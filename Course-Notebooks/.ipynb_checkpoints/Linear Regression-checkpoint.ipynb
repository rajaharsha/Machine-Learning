{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: <br>\n",
    "Data science basics<br>\n",
    "Doing Data Science<br>\n",
    "By: Cathy O'Neil; Rachel Schutt<br>\n",
    "Publisher: O'Reilly Media, Inc.<br>\n",
    "Pub. Date: October 24, 2013<br>\n",
    "Print ISBN-13: 978-1-4493-5865-5<br>\n",
    "Chapter 3 <br>\n",
    "http://proquest.safaribooksonline.com.proxy.lib.odu.edu/book/databases/9781449363871"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LInear Regression\n",
    "\n",
    "It is used when you want to express the mathematical relationship between two variables. When you use it, you are making the assumption that there is a linear relationship between an outcome variable (sometimes also called the response variable, dependent variable, or label) and a predictor (sometimes also called an independent variable, explanatory variable, or feature). Regression can be described as a statistical model as well as a machine learning algorithm.\n",
    "\n",
    "Motivating Example: Suppose you run a social networking site that charges a monthly subscription fee of $25. Each month you collect the following data:\n",
    "\n",
    "x: number of users (explanatory/independent variable)\n",
    "\n",
    "y: total revenues (dependent variable)\n",
    "\n",
    "$S = \\{ (x, y) = (1, 25), (10, 250), (100, 2500), (200, 5000) \\}$\n",
    "\n",
    "Relationship between x and y ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1, 10, 100, 200]\n",
    "y = [25, 250, 2500, 5000]\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have to plot to see the relationship between the data as $y = 25x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: We randomly sample the following data from a social networking site:\n",
    "\n",
    "x: number of friends\n",
    "\n",
    "y: time spent in seconds on the site\n",
    "<br>\n",
    "<font size=\"3\">\n",
    "$S = \\{ (x, y) = (7,276), (3,43), (4,82), (6,136), (10,417), (9,269), (3,60), (6,200) \\}$\n",
    "</font>\n",
    "<br><br>\n",
    "The relationship between x and y is not so obvious. Let us plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [7,3,4,6,10,9,3,6]\n",
    "y = [276,43,82,136,417,269,60,200]\n",
    "plt.plot(x, y, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is some kind of linear relationship - the more new friends you have, the more time you might spend on the site. In conatrast to the earlier example, there is no deterministics relationship - for the same value of $x$ there are multiple values of $y$ ($x = 3$ and $x =6$). \n",
    "\n",
    "How can we figure out the relationship here?\n",
    "\n",
    "I can draw multiple lines that seems to model the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [7,3,4,6,10,9,3,6]\n",
    "y = [276,43,82,136,417,269,60,200]\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([3, 10], [50, 380])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [7,3,4,6,10,9,3,6]\n",
    "y = [276,43,82,136,417,269,60,200]\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([3, 10], [50, 380])\n",
    "plt.plot([3, 10], [2, 400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we pick which one?\n",
    "\n",
    "Because we are assuming a linear relationship, let us assume we can express this in a general form:\n",
    "<br><br>\n",
    "<font size=\"3\">$y = \\beta_0 + \\beta_1 x $</font>\n",
    "\n",
    "<br>\n",
    "Now based on the observed data:\n",
    "\n",
    "<font size=\"3\">\n",
    "$S = \\{ (x, y) = (7,276), (3,43), (4,82), (6,136), (10,417), (9,269), (3,60), (6,200) \\}$\n",
    "</font>\n",
    "<br><br>\n",
    "\n",
    "We can express our model as:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    276  \\\\    \n",
    "    43 \\\\ \n",
    "    82 \\\\    \n",
    "    136 \\\\ \n",
    "    417 \\\\    \n",
    "    269 \\\\ \n",
    "    60 \\\\    \n",
    "    200  \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    1       & 7 \\\\    \n",
    "    1       & 3 \\\\ \n",
    "    1       & 4 \\\\    \n",
    "    1       & 6 \\\\ \n",
    "    1       & 10 \\\\    \n",
    "    1       & 9 \\\\ \n",
    "    1       & 3 \\\\    \n",
    "    1       & 6  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\beta_0 \\\\  \n",
    "    \\beta_1\n",
    "\\end{bmatrix}$$\n",
    "<br><br>\n",
    "In matrix notation:\n",
    "<br>\n",
    "<font size=\"4\">\n",
    "$$\\mathbf{\\hat{y}} = \\mathbf{X}\\mathbf{\\beta}$$\n",
    "</font>\n",
    "\n",
    "Here, $\\mathbf{X}$ is referred as a design matrix and in general it is of size $m \\times n$, where $m$ is the number of observations and $n$ is the number of features. The $\\mathbf{\\beta}$ is a vector of size $n$. The $\\mathbf{\\hat{y}}$ gives estimated value of $\\mathbf{y}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "So, how do we calculate  $\\mathbf{\\beta}$. The intuition behind linear regression is that you want to find the line (in this example) that minimizes the distance between all the points and the line. More specifically, linear regression seeks to find the line that minimizes the sum of the squares of the vertical distances between the approximated or predicted $\\hat{y_i}$s and the observed $y_i$s. This method is called least squares estimation and it minimizes the prediction error.\n",
    "\n",
    "The prediction error $R(\\beta_0, \\beta_1)$ is given by:\n",
    "\n",
    "$R = [276 -(\\beta_0 + 7\\beta_1)]^2 +  [43 -(\\beta_0 + 3\\beta_1)]^2 + [82 -(\\beta_0 + 4\\beta_1)]^2 + \\ldots $ \n",
    "\n",
    "Calculate partial derivatives with respect to $\\beta_0$ and $\\beta_1$ and set them to zero.\n",
    "\n",
    "$\\dfrac{\\partial R}{\\partial \\beta_0} = 0$, and $\\dfrac{\\partial R}{\\partial \\beta_1} = 0$\n",
    "\n",
    "This  result in two equations with two unknowns ($\\beta_0$ and $\\beta_1$). \n",
    "\n",
    "NOTE: In general this results in a <font color='red'>linear</font> system of equations with unknow $\\beta$'s. The \"Linear Model\" names come from here.\n",
    "\n",
    "Exercise (Take Home): Solve these equations for $\\beta_0$ and $\\beta_1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction error in matrix notation can be expressed as:\n",
    "\n",
    "$R =  (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) $\n",
    "\n",
    "We can simplify this\n",
    "\n",
    "$R =  (\\mathbf{y}^T - (\\mathbf{X}\\mathbf{\\beta})^T)(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) $\n",
    "\n",
    "$~~ =  (\\mathbf{y}^T - \\mathbf{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta}) $\n",
    "\n",
    "$~~ =  \\mathbf{y}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X}\\mathbf{\\beta} - \\mathbf{\\beta}^T\\mathbf{X}^T \\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T \\mathbf{X}\\mathbf{\\beta}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the transpose of a scalar is a scalar. Hence we write:\n",
    "\n",
    "$\\mathbf{y}^T \\mathbf{X}\\mathbf{\\beta} = (\\mathbf{y}^T \\mathbf{X}\\mathbf{\\beta})^T$\n",
    "\n",
    "This can be simplifed to:\n",
    "\n",
    "$\\mathbf{y}^T \\mathbf{x}\\mathbf{\\beta} = ( \\mathbf{X}\\mathbf{\\beta})^T \\mathbf{y} = \\mathbf{\\beta}^T\\mathbf{X}^T \\mathbf{y}$\n",
    "\n",
    "We can use this to simplify $R$\n",
    "\n",
    "$R =  \\mathbf{y}^T \\mathbf{y}  - 2\\mathbf{\\beta}^T\\mathbf{X}^T \\mathbf{y} + \\mathbf{\\beta}^T\\mathbf{X}^T \\mathbf{X}\\mathbf{\\beta}\n",
    "$\n",
    "<br><br>\n",
    "\n",
    "\n",
    "$\\dfrac{\\partial R}{\\partial \\mathbf{\\beta}} =- 2\\mathbf{X}^T \\mathbf{y} + 2 \\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} = 0\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\mathbf{X}^T \\mathbf{X}) \\mathbf{\\beta} =  \\mathbf{X}^T \\mathbf{y}$\n",
    "\n",
    "$(\\mathbf{X}^T \\mathbf{X})^{-1}(\\mathbf{X}^T \\mathbf{X}) \\mathbf{\\beta} =  (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}$\n",
    "\n",
    "$\\mathbf{\\beta} =  (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# solve using matrix notation\n",
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xt = np.vstack((np.ones(8), xt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Xt.T\n",
    "y = yt.T\n",
    "print (np.shape(X), np.shape(y))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XtX = np.dot(X.T, X)\n",
    "print(XtX)\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "Xty = np.dot(X.T, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.dot(XtX_inv, Xty)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [7,3,4,6,10,9,3,6]\n",
    "y = [276,43,82,136,417,269,60,200]\n",
    "plt.plot(x, y, 'ro')\n",
    "# for plotting regression line use two points\n",
    "y3 =  b[0] + b[1]*3\n",
    "y10 = b[0] + b[1]*10\n",
    "plt.plot([3, 10], [y3, y10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve for $\\mathbf{\\beta}$ without computing the inverse, but by solving the system.\n",
    "\n",
    "$(\\mathbf{X}^T \\mathbf{X}) \\mathbf{\\beta} =  \\mathbf{X}^T \\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn = np.linalg.solve(XtX, Xty)\n",
    "bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Consider the following $(x,y)$ dataset:\n",
    "        \n",
    "<br>\n",
    "<font size=\"3\">\n",
    "$S = \\{ (x, y) = (1,6), (2,5), (3,7), (4,10)\\}$\n",
    "</font>\n",
    "<br><br>\n",
    "As shown above using numpy estimate the linear regression parameters and plot the regression line along with the scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, we are not restricted to using a line as the model. We can use a polynomial to model it, for example:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$\n",
    "\n",
    "This model is still linear with respect to parameters, so we can perform the same computation as before for estimating the parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quadratic model \n",
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "xt2 = xt*xt\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(xt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = len(xt)\n",
    "X = np.array([np.ones(m), xt, xt2]).T\n",
    "y = yt.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XtX = np.dot(X.T, X)\n",
    "Xty = np.dot(X.T, y)\n",
    "betaHat = np.linalg.solve(XtX, Xty)\n",
    "print(betaHat)\n",
    "plt.figure(1)\n",
    "xx = np.linspace(0, 10, 100)\n",
    "xx2 = xx * xx\n",
    "yy = np.array(betaHat[0] + betaHat[1] * xx + betaHat[2] * xx2)\n",
    "plt.plot(xx, yy.T, color='b')\n",
    "plt.scatter(xt, yt, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with Multiple Variables (Features)\n",
    "\n",
    "A linear regression with three variables (features) can be expressed as\n",
    "<br><br>\n",
    "<font size=\"3\">\n",
    "$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_3 + \\beta_3 x_3  $\n",
    "</font>\n",
    "\n",
    "Rest of the math dealing with matrix formulation and for estimating parameters remains the same. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning and Gradient Descent \n",
    "When there are many features and many observations, the matrix in the linear solver becomes big and its computation complexity is high. An alternative method based on gradient descent can be much faster in such cases. Gradient descent and its variants stochastic gradient descent are very popular in machine learning community. We will be using terminology used by that community, so  you may see me using different names for the same stuff.\n",
    "\n",
    "Use  training data to learn parameters/weights of a model\n",
    "\n",
    "Use the model with learned parameters/weights to make prediction\n",
    "\n",
    "#### Training Data\n",
    "Training data is the same as the set of observations in our earlier example for linear regression.\n",
    "\n",
    "$S = \\{ (x, y) = (7,276), (3,43), (4,82), (6,136), (10,417), (9,269), (3,60), (6,200) \\}$\n",
    "\n",
    "Our training set consists of $m=8$ samples, where $i$th training sample is $(x^{(i)}, y^{(i)})$, $i=1,m$. In our example training data $x^{(1)} = 7$ and $y^{(1)} = 276$. Note that in the case of multivariable linear regression, $x^{(i)}$ has various components - as many as the number of features. The $j$th feature is represented by $x_j^{(i)}$\n",
    "\n",
    "The model we want to train is:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x $\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    276  \\\\    \n",
    "    43 \\\\ \n",
    "    82 \\\\    \n",
    "    136 \\\\ \n",
    "    417 \\\\    \n",
    "    269 \\\\ \n",
    "    60 \\\\    \n",
    "    200  \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    1       & 7 \\\\    \n",
    "    1       & 3 \\\\ \n",
    "    1       & 4 \\\\    \n",
    "    1       & 6 \\\\ \n",
    "    1       & 10 \\\\    \n",
    "    1       & 9 \\\\ \n",
    "    1       & 3 \\\\    \n",
    "    1       & 6  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\beta_0 \\\\  \n",
    "    \\beta_1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Using the new  notation and treating the first column of ones as $0$th feature,  we can express this as:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    y^1  \\\\    \n",
    "    y^2 \\\\ \n",
    "    y^3 \\\\    \n",
    "    y^4 \\\\ \n",
    "    y^5 \\\\    \n",
    "    y^6 \\\\ \n",
    "    y^7 \\\\    \n",
    "    y^8  \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    x_0^{(1)}       & x_1^{(1)}\\\\    \n",
    "    x_0^{(2)}       & x_1^{(2)}\\\\ \n",
    "    x_0^{(3)}       & x_1^{(3)}\\\\    \n",
    "    x_0^{(4)}       & x_1^{(4)}\\\\ \n",
    "    x_0^{(5)}       & x_1^{(5)}\\\\    \n",
    "    x_0^{(6)}       & x_1^{(6)}\\\\ \n",
    "    x_0^{(7)}       & x_1^{(7)}\\\\    \n",
    "    x_0^{(8)}       & x_1^{(8)} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\beta_0 \\\\  \n",
    "    \\beta_1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In machine learning the function that we want to learn is referred as hypotheses $h$, and in our case:\n",
    "\n",
    "$h_{\\beta}(x) = h(x) = \\beta_0 x_0 + \\beta_1 x_1$ (this is also sometime written as $h_{\\beta}(x) = \\mathbf{\\beta}^T \\mathbf{x}$, where $\\mathbf{x}$ is a feature vector)\n",
    "\n",
    "For a given set of parameters $\\beta_0$, $\\beta_1$, and the $i$th training data, the value of the function is given by:\n",
    "\n",
    "$h(x^{(i)}) = \\beta_0 x_0^{(i)} + \\beta_1 x_1^{(i)}$\n",
    "\n",
    "Now given the training set, how do we learn the parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function\n",
    "We define a cost function $J(\\beta)$ that measures how close the $h(x^{(i)})$'s are to  the corresponding $y^{(i)}$'s for a  given set of parameters $\\beta$. It is very similar to error expression defined earlier. \n",
    "\n",
    "$J(\\beta) = \\dfrac{1}{2}\\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2$\n",
    "\n",
    "Now we are ready to talk about gradient descent.\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "Gradient descent algorithm starts with some initial $\\beta_j$'s, and repeatedly update them:\n",
    "\n",
    "$\\beta_j := \\beta_j - \\alpha\\dfrac{\\partial}{\\partial \\beta_j} J(\\beta)$\n",
    "\n",
    "In our example, we are updating two parameters $\\beta_0$ and $\\beta_1$ at each time step. The $\\alpha$ is called learning rate. This algorithm basically takes a step in the direction of steepest decrease of $J$ for updating $\\beta_j$'s.\n",
    "\n",
    "We need the partial derivative, that is the direction of steepest decrease.\n",
    "\n",
    "$\\dfrac{\\partial}{\\partial \\beta_j} J(\\beta) = \\sum_{i=1}^m (h(x^{(i)})-y^{(i)})x_{j}^{(i)}$\n",
    "\n",
    "This gives\n",
    "\n",
    "$\\beta_j := \\beta_j - \\alpha\\sum_{i=1}^m (h(x^{(i)})-y^{(i)})x_{j}^{(i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Consider our example training set $S = \\{ (x, y) = (7,276), (3,43), (4,82), (6,136), (10,417), (9,269), (3,60), (6,200) \\}$. Starting with initial value of $\\beta_0 = 0.0$, and $\\beta_1=0.0$, update the parameters for one time step. Use $\\alpha = 0.005$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#partial solution\n",
    "\n",
    "def slope(b0, b1, X):\n",
    "    sumv0 = 0.0\n",
    "    sumv1 = 0.0\n",
    "    for i in range(8):\n",
    "        sumv0 = \n",
    "        sumv1 = \n",
    "       \n",
    "    return sumv0, sumv1\n",
    "\n",
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])\n",
    "m = len(xt)\n",
    "X = np.array([np.ones(m), xt]).T\n",
    "alpha = 0.005\n",
    "b0 = 0.0\n",
    "b1 = 0.0\n",
    "sum0, sum1 = slope(b0, b1, X)\n",
    "b0 = b0 - alpha*sum0\n",
    "b1 = b1 - alpha*sum1\n",
    "\n",
    "print (b0, b1)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Iterate over 1500 time steps and print out the final parameter values. Use the parameter values to compute y estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Algorithm\n",
    "\n",
    "In gradient descent at each time step we update parameter values using all the training data. If at each time step we use only one traning sample to update the parameter values, the algorithm becomes stochastic gradient algorithm. For very large training data and complex models (for example, neural networks), the stochastic gradient is more efficient. For machine learning community, the stochastic gradient is a preferred algorithm over gradient descent. \n",
    "\n",
    "The following expression shows how to update parameter values using a single traning sample $i$.\n",
    "\n",
    "$\\beta_j := \\beta_j - \\alpha(h(x^{(i)})-y^{(i)})x_{j}^{(i)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with initial value of $\\beta_0 = 0.0$, and $\\beta_1=0.0$, update the parameters for one time step using sample $(7, 276)$. Use $\\alpha = 0.005$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.005\n",
    "b0 = 0.0\n",
    "b1 = 0.0\n",
    "s0 = (b0*1.0 + b1*7.0 - 276)*1.0\n",
    "s1 = (b0*1.0 + b1*7.0 - 276)*7.0\n",
    "b0 = b0 - alpha*s0\n",
    "b1 = b1 - alpha*s1\n",
    "print (b0, b1, s0, s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Starting with initial value of $\\beta_0 = 0.0$, and $\\beta_1=0.0$, update the parameters for 8 time steps using 8 samples one after another. Use $\\alpha = 0.005$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Partial Solution\n",
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])\n",
    "m = len(xt)\n",
    "X = np.array([np.ones(m), xt]).T\n",
    "y = yt.T\n",
    "b0 = 0.0\n",
    "b1 = 0.0\n",
    "for i in range(8):\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Starting with initial value of $\\beta_0 = 0.0$, and $\\beta_1=0.0$, repeat the computation of previous exercise 700 times. Please make sure you intialize $\\beta_0 = 0.0$, and $\\beta_1=0.0$ only once at the beginning of the iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Higher degree polynomial\n",
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "xt2 = xt*xt\n",
    "xt3 = xt*xt*xt\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])\n",
    "m = len(xt)\n",
    "X = np.array([np.ones(m), xt, xt2, xt3]).T\n",
    "y = yt.T\n",
    "XtX = np.dot(X.T, X)\n",
    "Xty = np.dot(X.T, y)\n",
    "betaHat = np.linalg.solve(XtX, Xty)\n",
    "print(betaHat)\n",
    "plt.figure(1)\n",
    "xx = np.linspace(0, 10, 100)\n",
    "xx2 = xx * xx\n",
    "xx3 = xx * xx * xx\n",
    "yy = np.array(betaHat[0] + betaHat[1] * xx + betaHat[2] * xx2 + betaHat[3] * xx3)\n",
    "plt.plot(xx, yy.T, color='b')\n",
    "plt.scatter(xt, yt, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Higher degree polynomial\n",
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "xt2 = xt*xt\n",
    "xt3 = xt*xt*xt\n",
    "xt4 = xt*xt*xt*xt\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])\n",
    "m = len(xt)\n",
    "X = np.array([np.ones(m), xt, xt2, xt3, xt4]).T\n",
    "y = yt.T\n",
    "XtX = np.dot(X.T, X)\n",
    "Xty = np.dot(X.T, y)\n",
    "betaHat = np.linalg.solve(XtX, Xty)\n",
    "print(betaHat)\n",
    "plt.figure(1)\n",
    "xx = np.linspace(0, 10, 100)\n",
    "xx2 = xx * xx\n",
    "xx3 = xx * xx * xx\n",
    "xx4 = xx * xx * xx * xx\n",
    "yy = np.array(betaHat[0] + betaHat[1] * xx + betaHat[2] * xx2 + betaHat[3] * xx3 + betaHat[4] * xx4)\n",
    "plt.plot(xx, yy.T, color='b')\n",
    "plt.scatter(xt, yt, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot the algorithm is aggresively trying to fit every data point, and is an example of overfitting. These algorithm typically do not perform well in test/prediction phase. On the other extreme is underfitting (when we used one feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "L2 regularization (Ridge regression) adds a term to cost function so as to keep parameter value associated with less important feature to be small.\n",
    "\n",
    "$J(\\beta) = \\dfrac{1}{2}\\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2   +   \\lambda \\sum_{j=1}^n \\beta_j^2 $\n",
    "\n",
    "L1 regularization (Lasso Regression)\n",
    "\n",
    "$J(\\beta) = \\dfrac{1}{2}\\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2   +   \\lambda \\sum_{j=1}^n \\lvert \\beta_j \\rvert $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear Regression using Scikit Package\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])\n",
    "X = xt[:, np.newaxis] # another way to convert row to col vector\n",
    "y = yt[:, np.newaxis]\n",
    "print(np.shape(y), np.shape(X_plot))\n",
    "plt.scatter(xt, yt, color='navy', s=30, marker='o')\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "# to plot regression line create sample x data and estimate y using prediction\n",
    "x_plot = np.linspace(0, 10, 10)\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "y_plot = regr.predict(X_plot)\n",
    "plt.plot(x_plot, y_plot, color='teal',linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using higher degree polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])\n",
    "X = xt[:, np.newaxis] \n",
    "y = yt[:, np.newaxis]\n",
    "degree = 2\n",
    "poly = PolynomialFeatures(degree)\n",
    "XP = poly.fit_transform(X)\n",
    "#print (X)\n",
    "#print (XP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to plot regression line create sample x data\n",
    "x_plot = np.linspace(0, 10, 50)\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "print (X_plot[:5])\n",
    "XP_plot = poly.fit_transform(X_plot)\n",
    "print (XP_plot[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(xt, yt, color='navy', s=30, marker='o', label=\"training points\")\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(XP, y)\n",
    "y_plot = regr.predict(XP_plot) # estimate y for test data for plotting\n",
    "plt.plot(x_plot, y_plot, color='teal', linewidth=2,label=\"degree %d\" % degree)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all plots in a loop with different degree of polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])\n",
    "X = xt[:, np.newaxis] \n",
    "y = yt[:, np.newaxis]\n",
    "x_plot = np.linspace(0, 10, 50)\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "plt.scatter(xt, yt, color='navy', s=30, marker='o', label=\"training points\")\n",
    "\n",
    "colors = ['black', 'teal', 'yellowgreen', 'gold']\n",
    "for count, degree in enumerate([1,2,3,4]):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    XP = poly.fit_transform(X)\n",
    "    XP_plot = poly.fit_transform(X_plot)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(XP, y)\n",
    "    y_plot = regr.predict(XP_plot) # estimate y for test data for plotting\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=1,label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with L2 regularization (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt = np.array([7,3,4,6,10,9,3,6])\n",
    "yt = np.array([276, 43, 82, 136, 417, 269, 60, 200])\n",
    "X = xt[:, np.newaxis] \n",
    "y = yt[:, np.newaxis]\n",
    "x_plot = np.linspace(0, 10, 50)\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "plt.scatter(xt, yt, color='navy', s=30, marker='o', label=\"training points\")\n",
    "\n",
    "colors = ['black', 'teal', 'yellowgreen', 'gold']\n",
    "for count, degree in enumerate([1,2,3,4]):\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    XP = poly.fit_transform(X)\n",
    "    XP_plot = poly.fit_transform(X_plot)\n",
    "    regr = Ridge(alpha=1.0)\n",
    "    regr.fit(XP, y)\n",
    "    y_plot = regr.predict(XP_plot) # estimate y for test data for plotting\n",
    "    plt.plot(x_plot, y_plot, color=colors[count], linewidth=1,label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: For the following dataset from scikit repeat the \"Create all plots in a loop with different degree of polynomials\" and \"Linear model with L2 regularization (L2)\". For the second set of plot, use different values of alpha and degree to find the best fit (visually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "# Use only one feature\n",
    "X = diabetes.data[:,2]\n",
    "y = diabetes.target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
